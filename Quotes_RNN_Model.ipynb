{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quotes_RNN_Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn0FG81KBZeP"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT-JIlC0yrRb",
        "outputId": "3da8f813-5807-4574-af1d-3a94393dbb5c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auKEQQwsy6xX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-2bXLjgzCND",
        "outputId": "fb2f1ed0-dfe0-42ab-8728-126d83187169"
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtP8p-an_cVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db82d37-2bb9-4606-941e-f01782588747"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAUtr_LVzRIZ"
      },
      "source": [
        "train = pd.read_csv('drive/MyDrive/train_quotes.csv')\n",
        "test = pd.read_csv('drive/MyDrive/test_quotes.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2SfB07O_bn3"
      },
      "source": [
        "train = train.drop(columns = ['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "test = test.drop(columns = ['Unnamed: 0', 'Unnamed: 0.1'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "cjKFvwZqCEAT",
        "outputId": "917cceca-aff0-47c1-f4ff-3d4b0227918a"
      },
      "source": [
        "train.head(1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Quote</th>\n",
              "      <th>Likes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>She turned to look at him, and he was already ...</td>\n",
              "      <td>210</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Quote  Likes\n",
              "0  She turned to look at him, and he was already ...    210"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "_d5JDWCyCWwc",
        "outputId": "b93d7efb-928f-48df-8f46-e0edbb2043e5"
      },
      "source": [
        "test.head(1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Quote</th>\n",
              "      <th>Likes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyond work and love, I would add two other in...</td>\n",
              "      <td>350</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Quote  Likes\n",
              "0  Beyond work and love, I would add two other in...    350"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EosYfiZByW7"
      },
      "source": [
        "# Preprocessing - to remove length-1 words, and remove non-alphabet symbols\n",
        "def preprocessing(quotes):\n",
        "\n",
        "    processed_quotes = []\n",
        "    \n",
        "    for quote in tqdm(quotes):\n",
        "        \n",
        "        # remove other non-alphabets symbols with space (i.e. keep only alphabets and whitespaces).\n",
        "        processed = re.sub('[^a-zA-Z ]', '', quote)\n",
        "        \n",
        "        words = processed.split()\n",
        "        \n",
        "        # keep words that have length of more than 1 (e.g. gb, bb), remove those with length 1.\n",
        "        processed_quotes.append(' '.join([word for word in words if len(word) > 1]))\n",
        "    \n",
        "    return processed_quotes"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJHOW7VsCISA",
        "outputId": "04c52026-5c01-44a1-8c7c-25c36166acf1"
      },
      "source": [
        "train['Quote'] = preprocessing(train['Quote'])\n",
        "test['Quote'] = preprocessing(test['Quote'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 56617/56617 [00:00<00:00, 89198.01it/s]\n",
            "100%|██████████| 14155/14155 [00:00<00:00, 91637.35it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-z5C98nCjR4"
      },
      "source": [
        "def preprocessing_2(quotes):\n",
        "    \n",
        "    processed_quotes = []\n",
        "\n",
        "    for quote in tqdm(quotes):\n",
        "        tokens = word_tokenize(quote)\n",
        "        # Convert to lower case\n",
        "        tokens = [w.lower() for w in tokens]\n",
        "        # Remove punctuation\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [w.translate(table) for w in tokens]\n",
        "        # Remove remaining tokens that are not alphabetic\n",
        "        words = [word for word in stripped if word.isalpha()]\n",
        "        # Filter out stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [w for w in words if not w in stop_words]\n",
        "        \n",
        "        processed_quotes.append(words)\n",
        "        \n",
        "    return processed_quotes"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GyNjMkcDYHY",
        "outputId": "dae42852-1f41-4734-a812-572c1b0bf30d"
      },
      "source": [
        "train['Quote'] = preprocessing_2(train['Quote'])\n",
        "test['Quote'] = preprocessing_2(test['Quote'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 56617/56617 [00:22<00:00, 2511.07it/s]\n",
            "100%|██████████| 14155/14155 [00:05<00:00, 2525.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGAxjktkD53b"
      },
      "source": [
        "# Shuffle test again, and reset index (very important!!!)\n",
        "test = test.sample(frac = 1)\n",
        "test = test.reset_index(drop = True)\n",
        "train = train.reset_index(drop = True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRTEWa1_EsQJ"
      },
      "source": [
        "# Fitting Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjwuEUqgBg13"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYD0udbnEXan"
      },
      "source": [
        "# Extract the embeddings from the stored file\n",
        "# Embedding is size 111k (# words) x 100 (dimensions)\n",
        "import os \n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('drive/MyDrive/word2vec_train.txt'), encoding = 'utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juapCaSeEuGc"
      },
      "source": [
        "def vectorize_text(content):\n",
        "\n",
        "  # Vectorize the text samples into 2D integer tensor - max length 16 words\n",
        "  tokenizer_obj = Tokenizer()\n",
        "\n",
        "  # Fit the tokenizer on the text\n",
        "  tokenizer_obj.fit_on_texts(content)\n",
        "\n",
        "  # Generate the sequence of tokens\n",
        "  sequences = tokenizer_obj.texts_to_sequences(content)\n",
        "\n",
        "  # Get the max length of each article - 5587\n",
        "  max_length = max([len(s) for s in content])\n",
        "  \n",
        "  # Pad the sequences\n",
        "  vectorized_text = pad_sequences(sequences, maxlen = max_length)\n",
        "\n",
        "  return vectorized_text, tokenizer_obj, max_length"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXZcR5Lc_noU"
      },
      "source": [
        "def get_embedding_matrix(tokenizer_obj, EMBEDDING_DIM = 100):\n",
        " \n",
        "  word_index = tokenizer_obj.word_index\n",
        "\n",
        "  num_words = len(word_index) + 1\n",
        "  words_not_found = []\n",
        "  # Create the emedding matrix - map embeddings from word2vec model for each word and create matrix of word vectors\n",
        "  embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "  for word, i in word_index.items():\n",
        "      if i > num_words: # Least common words (don't care)\n",
        "          continue\n",
        "          \n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      \n",
        "      if (embedding_vector is not None):\n",
        "          # Assign the ith elmenet of the embedding matrix to the embedding of that word\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "      else:\n",
        "          words_not_found.append(word)\n",
        "          \n",
        "  print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "  return embedding_matrix"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-gnE8v1AMwS",
        "outputId": "cdaa7b3d-80d5-455f-b368-61d68117d55d"
      },
      "source": [
        "# Vectorize the text (return document x length matrix)\n",
        "train_vectorized, tokenizer, max_length = vectorize_text(train['Quote'])\n",
        "\n",
        "test_vectorized = tokenizer.texts_to_sequences(test['Quote'])\n",
        "test_vectorized = pad_sequences(test_vectorized, maxlen = max_length)\n",
        "\n",
        "# Get the embedding matrix of the words\n",
        "embedding_matrix = get_embedding_matrix(tokenizer)\n",
        "num_words = embedding_matrix.shape[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of null word embeddings: 5645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTwBENmDBxto"
      },
      "source": [
        "# Training Deep Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o0wrDLeBFFZ"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Embedding, LSTM, GRU, SpatialDropout1D, Bidirectional, Dropout, BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import SGD, Adam\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r4Po87FCMKn"
      },
      "source": [
        "# Original RNN Model\n",
        "def RNN_Model():\n",
        "    \n",
        "    text_sequence = Input(shape = (max_length,), name = 'text_sequence_input')\n",
        "    rnn_layer = Embedding(num_words, EMBEDDING_DIM, weights = [embedding_matrix], trainable = False, name = 'embedding')(text_sequence)\n",
        "    rnn_layer = LSTM(units = 32, dropout = 0.1)(rnn_layer)\n",
        "    rnn_layer = Dense(32, activation = 'relu')(rnn_layer)\n",
        "    output = Dense(1, name = 'output')(rnn_layer)\n",
        "    model = Model(inputs = text_sequence, outputs = output)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAGYikq6J9qU",
        "outputId": "ad5d5830-22e8-4655-fa26-955c283779eb"
      },
      "source": [
        "model = RNN_Model()\n",
        "model.summary()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text_sequence_input (InputLa [(None, 373)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 373, 100)          8969000   \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 32)                17024     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 8,987,113\n",
            "Trainable params: 18,113\n",
            "Non-trainable params: 8,969,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtrWAGyeogVF",
        "outputId": "a06feb43-5e9a-4fa3-91a8-9fb220787585"
      },
      "source": [
        "# Establish X and y data\n",
        "X_train = train_vectorized\n",
        "X_test = test_vectorized\n",
        "\n",
        "y_train = train['Likes'].to_numpy()\n",
        "y_test = test['Likes'].to_numpy()\n",
        "\n",
        "print('Shape of X_train: ', X_train.shape)\n",
        "print('Shape of y_train: ', y_train.shape)\n",
        "print('Shape of X_test: ', X_test.shape)\n",
        "print('Shape of y_test: ', y_test.shape)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train:  (56617, 373)\n",
            "Shape of y_train:  (56617,)\n",
            "Shape of X_test:  (14155, 373)\n",
            "Shape of y_test:  (14155,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqypf7yTM6CM"
      },
      "source": [
        "model.compile(loss = keras.losses.mean_squared_error, optimizer = Adam(learning_rate = 0.001))"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxGSHHO9nqG_",
        "outputId": "4dca67f3-d252-44a2-d88d-ce5238ffef70"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 32, epochs = 20, verbose = 1)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1770/1770 [==============================] - 27s 14ms/step - loss: 4184354.6778\n",
            "Epoch 2/20\n",
            "1770/1770 [==============================] - 25s 14ms/step - loss: 4168402.6861\n",
            "Epoch 3/20\n",
            "1770/1770 [==============================] - 25s 14ms/step - loss: 4052671.8979\n",
            "Epoch 4/20\n",
            "1770/1770 [==============================] - 24s 14ms/step - loss: 3439957.4015\n",
            "Epoch 5/20\n",
            "1770/1770 [==============================] - 25s 14ms/step - loss: 3339018.9597\n",
            "Epoch 6/20\n",
            "1770/1770 [==============================] - 24s 14ms/step - loss: 3390060.5394\n",
            "Epoch 7/20\n",
            "1770/1770 [==============================] - 24s 14ms/step - loss: 4190985.4906\n",
            "Epoch 8/20\n",
            "1770/1770 [==============================] - 24s 14ms/step - loss: 4059141.4344\n",
            "Epoch 9/20\n",
            "1770/1770 [==============================] - 25s 14ms/step - loss: 5287578.2567\n",
            "Epoch 10/20\n",
            "1770/1770 [==============================] - 24s 14ms/step - loss: 3473090.8803\n",
            "Epoch 11/20\n",
            "1770/1770 [==============================] - 25s 14ms/step - loss: 3283514.1142\n",
            "Epoch 12/20\n",
            "1770/1770 [==============================] - 24s 14ms/step - loss: 3612584.1462\n",
            "Epoch 13/20\n",
            "1770/1770 [==============================] - 25s 14ms/step - loss: 3718218.5474\n",
            "Epoch 14/20\n",
            "1770/1770 [==============================] - 25s 14ms/step - loss: 4717129.1296\n",
            "Epoch 15/20\n",
            "1770/1770 [==============================] - 25s 14ms/step - loss: 4132334.1032\n",
            "Epoch 16/20\n",
            "1770/1770 [==============================] - 25s 14ms/step - loss: 4591986.0050\n",
            "Epoch 17/20\n",
            "1770/1770 [==============================] - 24s 14ms/step - loss: 3565984.7205\n",
            "Epoch 18/20\n",
            "1770/1770 [==============================] - 24s 14ms/step - loss: 3519889.9595\n",
            "Epoch 19/20\n",
            "1770/1770 [==============================] - 24s 14ms/step - loss: 4682498.6969\n",
            "Epoch 20/20\n",
            "1770/1770 [==============================] - 24s 14ms/step - loss: 3438680.1420\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ_h9n7dzUSR",
        "outputId": "c089a882-cd61-41c5-9d0e-da3f0b87cb1f"
      },
      "source": [
        "# Evaluate on test set\n",
        "results = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "443/443 [==============================] - 4s 8ms/step - loss: 7717539.5000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}